---
layout: post
title: "The Ethics of Artificial Intelligence"
tags: [Software, Machine Learning, Artificial Intelligence, Ethics, Parenting]
summary: Running a program that is self-aware means becoming a parent.
---
{% include JB/setup %}

## The Current State of AI

In recent years, enormous progress has been made in <a href="https://en.wikipedia.org/wiki/Machine_learning" targe="_blank" title="ML: an Extremely Broad Field in Computer Science, including AI">Machine Learning</a>, but thankfully little progress has been made on creating human-level <a href="https://en.wikipedia.org/wiki/Artificial_intelligence" targe="_blank" title="AI: a tiny, subfield of ML, still in its infancy">Artificial Intelligence</a> in software.  I say "thankfully" because the the ethics of creating true AI are rarely discussed, particularly among the private organizations that spend the most money on ML.

NOTE: This next paragraphs sucks, re-write it.

Science Fiction writers love to warn us about the future of AI: dramatic battles between humans and robotic war machines. And while it is possible that fully-autonomous, human-level AI competing for the same natural resources as humans, that is a far distant concern compared to the ethical dilemmas and tragedies that will occur for many years before that point.

The first serious ethical dilemmas of machine learning are just now starting to be felt. Human beings are being replaced in vast numbers by software routines that can crudely learn to do basic tasks. Billions of dollars are being spent in America alone to see which company can develop the first full autonomous vehicles to replace truck drivers: which is currently the highest-paying profession for the non-college-educated population. As this makes up one to two percent of the work force it seems that Google, Uber, and Ford are spending a lot more time trying to *win* this next big-cash race than they are the effect this will have on society.

If self-driving cars are as promised, they will irrevocably change the economy of the world. That is a real, economic shift that we are told you will see in your lifetime. But that is just machine learning. Artificial Intelligence (AI) won't just put people out of work (though presumably it would), it will 


## look to evolution

Artificial Intelligence will not suddenly become human-level one day, with Hal 9000 talking to his programmers. A good first step would be to create an AI that was as truly independent and clever as a fruit fly [fact check], which has just 260 neurons in its semi-brain. Then one day humans will create AI as clever as a lizard, then a house cat, a dog, a monkey, and after such progress continues, one day, human-level intelligence.

Remember that it took nearly 500 million years [fact check] from the birth of the first life on Earth for evolution to create human-level intelligence. Evolution is an extremely competitive and dynamic process, so it must take ...

What will AI look like?   It will take a long time to get there. Bugs, lizards, mammal-level, then human-level AI

Mental handicaps will happen


## the real problem

You are now a parent. Do not turn of the machine or change the program. You will be murdering your child

Corporations as parents?


## Will Human-Level AI by Self-Aware

No one knows, yet.  But it seems like we will find out soon.  And this is the crux of the matter.

Let's talk about a limited definition of "sentience".  ...Do so...

If you can create an intelligence as clever as a human, you would expect that it would be able to reason about its own existence.

After all, how intelligent can you be if you're not even smart enough to understand that *yo

## will human-level AI help?

The gift of humanity that has made us so successful is not our broad intelligence. After all, we already have machine learning that can identify handwriting and pictures of cats better than we can.  It is our self-awareness and sentience that propelled us to dominating the world as a species.

But what good will a self-aware program be? It certainly won't be any faster at recognizing hand-written digits or solving whatever mundane problems we want machines to do for us.  A self-aware program will, like us, waste most of its compute cycles on thinking about itself and worry about its lot in life.  Building a machine that can worry and get depressed will not help us automate factories or make driving safer.  More importantly, it will not help the machine solve problems faster, the only benefit to self-awareness is self-preservation. And is that a goal we want to instill in our machines?
